{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping News URL from Detik.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import bs4\n",
    "import json\n",
    "import time\n",
    "import timeit\n",
    "import threading\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "from datetime import date, timedelta, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datelist_generator(from_date,to_date,format_output=\"%d/%m/%Y\"):\n",
    "    dates = []\n",
    "    step = timedelta(days=1)\n",
    "    while from_date<=to_date:\n",
    "        # print(from_date)\n",
    "        dates.append(from_date.strftime(format_output))\n",
    "        from_date+=step\n",
    "    return dates\n",
    "\n",
    "def reformat_date_to_str(tobe_reformat_date,format_output):\n",
    "    return tobe_reformat_date.strftime(format_output)\n",
    "\n",
    "\n",
    "def get_soup(link_url):\n",
    "    htmltext = requests.get(link_url).text\n",
    "    soup = bs4.BeautifulSoup(htmltext,'html.parser')\n",
    "    return soup\n",
    "\n",
    "def get_max_page(soup):\n",
    "    max_page = 0\n",
    "    for element_pagination in soup.find_all(name=\"a\",attrs={\"class\":\"pagination__item itp-pagination\"}):\n",
    "        if re.match(r\"\\d+\",element_pagination.string):\n",
    "            if max_page<int(element_pagination.string):\n",
    "                max_page = int(element_pagination.string)\n",
    "    print(f\"max_page: {max_page}\")\n",
    "    return max_page\n",
    "\n",
    "def get_n_news_perpage(soup):\n",
    "    return len(soup.find_all(\"article\"))\n",
    "\n",
    "def collect_urls_perpage(soup,n_news_perpage):\n",
    "    urls = []\n",
    "    for l in range(n_news_perpage):\n",
    "        li_url = soup.find_all(\"article\")[l].find('a').get(\"href\")\n",
    "        urls.append(li_url)\n",
    "    return urls\n",
    "\n",
    "def get_urls_task(page_i,keyword,from_date,to_date):\n",
    "    template_i = f\"https://www.detik.com/search/searchnews?query={keyword}&page={page_i}&result_type=relevansi&siteid=3&fromdatex={from_date}&todatex={to_date}\"    \n",
    "    soup_i = get_soup(template_i)\n",
    "    n_news_perpage = get_n_news_perpage(soup_i)\n",
    "    urls = collect_urls_perpage(soup_i,n_news_perpage)\n",
    "    urls = list(set(urls))\n",
    "    return urls\n",
    "\n",
    "def detik_page_url_generator(from_date:str,to_date:str,keyword:str):\n",
    "    \"\"\"\n",
    "    - one keyword\n",
    "    - format from_data and to_date in string 'dd/mm/yyyy'\n",
    "    \"\"\"\n",
    "    page=1\n",
    "    template = f\"https://www.detik.com/search/searchnews?query={keyword}&page={page}&result_type=relevansi&siteid=3&fromdatex={from_date}&todatex={to_date}\"\n",
    "    soup = get_soup(template)\n",
    "    max_page = get_max_page(soup)\n",
    "    \n",
    "    # synchronous\n",
    "    # for page_i in range(1,max_page+1):\n",
    "    #     template_i = f\"https://www.detik.com/search/searchnews?query={keyword}&page={page_i}&result_type=relevansi&siteid=3&fromdatex={from_date}&todatex={to_date}\"    \n",
    "    #     soup_i = get_soup(template_i)\n",
    "    #     n_news_perpage = get_n_news_perpage(soup_i)\n",
    "    #     urls = collect_urls_perpage(soup_i,n_news_perpage)\n",
    "    #     urls = list(set(urls))\n",
    "        # urls_all.extend(urls)\n",
    "\n",
    "    # asynchronous\n",
    "    start_time = timeit.default_timer()\n",
    "    semaphore = threading.BoundedSemaphore(4)\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        future_task = [executor.submit(get_urls_task, page_i=page_i,keyword=keyword,from_date=from_date,to_date=to_date) for page_i in range(1,max_page+1)] \n",
    "        result_task = [future.result() for future in concurrent.futures.as_completed(future_task)]\n",
    "    end_time = timeit.default_timer()\n",
    "    urls_all = []\n",
    "    for urls_perpage in result_task:\n",
    "        urls_all.extend(urls_perpage)\n",
    "    urls_all = list(set(np.array(urls_all)))\n",
    "    print(f\"Finished collect all url, total : {len(urls_all)}, {end_time - start_time:.2f}s\")\n",
    "    return urls_all\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_page: 356\n",
      "Finished collect all url, total : 3234, 78.58s\n"
     ]
    }
   ],
   "source": [
    "urls_all = detik_page_url_generator(from_date=\"01/01/2024\",to_date=\"29/08/2024\",keyword=\"ekonomi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/============/demo/============/\n",
      "max_page: 88\n",
      "Finished collect all url, total : 796, 17.32s\n",
      "/============/anies/============/\n",
      "max_page: 454\n",
      "Finished collect all url, total : 3998, 96.35s\n",
      "/============/gempa/============/\n",
      "max_page: 82\n",
      "Finished collect all url, total : 799, 14.52s\n",
      "/============/ekonomi/============/\n",
      "max_page: 356\n",
      "Finished collect all url, total : 3248, 77.18s\n",
      "/============/politik/============/\n",
      "max_page: 841\n",
      "Finished collect all url, total : 6891, 168.57s\n",
      "/============/teknologi/============/\n",
      "max_page: 196\n",
      "Finished collect all url, total : 1837, 34.74s\n",
      "/============/olahraga/============/\n",
      "max_page: 70\n",
      "Finished collect all url, total : 657, 14.31s\n",
      "/============/wisata/============/\n",
      "max_page: 85\n",
      "Finished collect all url, total : 820, 16.44s\n",
      "/============/musik/============/\n",
      "max_page: 42\n",
      "Finished collect all url, total : 416, 7.35s\n",
      "/============/film/============/\n",
      "max_page: 43\n",
      "Finished collect all url, total : 422, 8.04s\n",
      "Total URL: 19884\n",
      "Unique url collected: 16643\n"
     ]
    }
   ],
   "source": [
    "keywords = ['demo','anies','gempa','ekonomi','politik','teknologi','olahraga','wisata','musik','film']\n",
    "\n",
    "all_keywords_urls_dict = {}\n",
    "all_keywords_urls_list = []\n",
    "for keyword in keywords:\n",
    "    print(f\"/============/{keyword}/============/\")\n",
    "    urls_keyword = detik_page_url_generator(from_date=\"01/01/2024\",to_date=\"29/08/2024\",keyword=keyword)\n",
    "    all_keywords_urls_list.extend(urls_keyword)\n",
    "    all_keywords_urls_dict[keyword] = urls_keyword\n",
    "\n",
    "print(f\"Total URL: {len(all_keywords_urls_list)}\")\n",
    "\n",
    "uniq_urls = list(set(all_keywords_urls_list))\n",
    "print(f\"Unique url collected: {len(uniq_urls)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "from newspaper.utils import BeautifulSoup\n",
    "\n",
    "def get_news(url):\n",
    "    article = Article(f'{url}','id')\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    authors = \", \".join(article.authors)\n",
    "    title = article.title\n",
    "    publish_date = article.publish_date.strftime(\"%Y-%m-%d %H:%M\")\n",
    "    meta_site_name = article.meta_site_name\n",
    "    meta_description = article.meta_description\n",
    "    meta_keywords = \", \".join(article.meta_keywords)\n",
    "    text = title+'\\n'+article.text\n",
    "    # summary = article.summary\n",
    "    return {\n",
    "            \"authors\" : authors,\n",
    "            \"title\" : title,\n",
    "            \"publish_date\" : publish_date,\n",
    "            \"meta_site_name\" : meta_site_name,\n",
    "            \"meta_description\" : meta_description,\n",
    "            \"meta_keywords\" : meta_keywords,\n",
    "            \"text\" : text,\n",
    "            # \"summary\" : summary,\n",
    "        }\n",
    "\n",
    "def get_all_news(urls):\n",
    "    start_time = timeit.default_timer()\n",
    "    semaphore = threading.BoundedSemaphore(4)\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        future_task = [executor.submit(get_news, url=url) for url in urls] \n",
    "        result_task = [future.result() for future in concurrent.futures.as_completed(future_task)]\n",
    "    end_time = timeit.default_timer()\n",
    "    # result_task = list(set(np.array(result_task).flatten().tolist())) # get unique\n",
    "    news_all = []\n",
    "    for news in result_task:\n",
    "        news_all.extend(news)\n",
    "    news_all = list(set(np.array(news_all)))\n",
    "    print(f\"Finished collect all url, total : {len(news_all)}, {end_time - start_time:.2f}s\")\n",
    "    return news_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = pd.DataFrame(get_all_news(uniq_urls))\n",
    "df_news.to_parquet('detiknews.parquet',engine='fastparquet')\n",
    "df_news.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
